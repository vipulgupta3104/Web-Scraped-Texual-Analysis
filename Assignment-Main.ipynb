{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e734b-46d7-4a1c-a5e4-d21d4eb3158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing basic and necessary python libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306cf935-8061-4276-be2f-a807392e3174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Netclan20241159</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Netclan20241160</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Netclan20241161</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Netclan20241162</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Netclan20241163</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              URL_ID                                                URL\n",
       "0    Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...\n",
       "1    Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...\n",
       "2    Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...\n",
       "3    Netclan20241020  https://insights.blackcoffer.com/efficient-pro...\n",
       "4    Netclan20241021  https://insights.blackcoffer.com/development-o...\n",
       "..               ...                                                ...\n",
       "142  Netclan20241159  https://insights.blackcoffer.com/population-an...\n",
       "143  Netclan20241160  https://insights.blackcoffer.com/google-lsa-ap...\n",
       "144  Netclan20241161  https://insights.blackcoffer.com/healthcare-da...\n",
       "145  Netclan20241162  https://insights.blackcoffer.com/budget-sales-...\n",
       "146  Netclan20241163  https://insights.blackcoffer.com/amazon-buy-bo...\n",
       "\n",
       "[147 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the input data\n",
    "df = pd.read_excel(\"Question/Input.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a4cb4-0dee-46e9-bd7a-ed91986ff856",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a10955-5777-42cf-8494-eb2ade82b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract text from URL using beautiful soup library\n",
    "\n",
    "def Data_Extraction(URL_ID,URL):\n",
    "\n",
    "    # importing URL\n",
    "    import requests\n",
    "    webpage=requests.get(URL).text\n",
    "\n",
    "    #using beautiful soup library for web scraping\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup=BeautifulSoup(webpage,'lxml')\n",
    "    article_text=soup.find_all('div',class_='td-post-content tagdiv-type')\n",
    "\n",
    "    #Removing HTML tags from the text extracted\n",
    "    import re \n",
    "    def remove_html_tags(text):\n",
    "        pattern=re.compile('<.*?>')\n",
    "        text = pattern.sub(r' ',str(text))\n",
    "        return text\n",
    "    final_text = remove_html_tags(article_text)\n",
    "\n",
    "    #Saving Extracted text into a text file with URL_ID as its filename\n",
    "    with open(f'Extracted Data/{URL_ID}.text','w') as file:\n",
    "        for word in final_text.split():\n",
    "            file.write(word+\"\\n\")\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05889958-2a8c-4fc9-8f4e-0b961fa2f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating article_text column in the data frame as well along with extracting data and storing it in external file as well\n",
    "df['article_text']=None\n",
    "\n",
    "for i in range(0,df.shape[0]):\n",
    "    URL_ID = df['URL_ID'][i]\n",
    "    URL = df['URL'][i]\n",
    "    \n",
    "    df['article_text'][i]=''.join(Data_Extraction(URL_ID,URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e297c96-5496-4928-bf41-013911402bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding of the Question/StopWords/StopWords_Auditor.txt is: ascii\n",
      "The encoding of the Question/StopWords/StopWords_Currencies.txt is: ISO-8859-1\n",
      "The encoding of the Question/StopWords/StopWords_DatesandNumbers.txt is: ascii\n",
      "The encoding of the Question/StopWords/StopWords_Generic.txt is: ascii\n",
      "The encoding of the Question/StopWords/StopWords_GenericLong.txt is: ascii\n",
      "The encoding of the Question/StopWords/StopWords_Geographic.txt is: ascii\n",
      "The encoding of the Question/StopWords/StopWords_Names.txt is: ascii\n"
     ]
    }
   ],
   "source": [
    "#Determining encoding type of text files for opening file in wight way before extracting stopwords\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        detector = chardet.universaldetector.UniversalDetector()\n",
    "        for line in file:\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "        detector.close()\n",
    "    encoding = detector.result['encoding']\n",
    "    print(f'The encoding of the {file_path} is: {encoding}')\n",
    "\n",
    "file_path = ['Question/StopWords/StopWords_Auditor.txt',\n",
    "             'Question/StopWords/StopWords_Currencies.txt',\n",
    "             'Question/StopWords/StopWords_DatesandNumbers.txt',\n",
    "             'Question/StopWords/StopWords_Generic.txt',\n",
    "             'Question/StopWords/StopWords_GenericLong.txt',\n",
    "             'Question/StopWords/StopWords_Geographic.txt',\n",
    "             'Question/StopWords/StopWords_Names.txt'\n",
    "            ]\n",
    "\n",
    "for i in file_path:\n",
    "    detect_encoding(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8d39a1-cf5f-4985-bcf6-4ee0390770ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the files are in two types of encoding, we are forming two functions for extracting stopwords, one for each encoding type\n",
    "\n",
    "def load_stopwords_ascii(file_path):\n",
    "    with open(file_path, 'r',encoding='ascii') as file:\n",
    "        stopwords=set(file.read().splitlines())  #Adding to set for fast lookup\n",
    "    return stopwords\n",
    "\n",
    "def load_stopwords_ISO(file_path):\n",
    "    with open(file_path, 'r',encoding='ISO-8859-1') as file:\n",
    "        stopwords=set(file.read().splitlines())  #Adding to set for fast lookup\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83fb929b-1078-4dd8-b38a-0cf5571c2f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stopwords extracted from files : 12768\n"
     ]
    }
   ],
   "source": [
    "#function to extract stopwords \n",
    "def extract_stopwords():\n",
    "\n",
    "    file_paths_ISO=['Question/StopWords/StopWords_Currencies.txt']\n",
    "    file_paths_ascii=['Question/StopWords/StopWords_Auditor.txt',\n",
    "                      'Question/StopWords/StopWords_DatesandNumbers.txt',\n",
    "                      'Question/StopWords/StopWords_Generic.txt',\n",
    "                      'Question/StopWords/StopWords_GenericLong.txt',\n",
    "                      'Question/StopWords/StopWords_Geographic.txt',\n",
    "                      'Question/StopWords/StopWords_Names.txt'\n",
    "                     ]\n",
    "\n",
    "    stopwords=set()\n",
    "    for i in file_paths_ISO:\n",
    "        stopwords=stopwords.union(load_stopwords_ISO(i))\n",
    "    \n",
    "    for i in file_paths_ascii:\n",
    "        stopwords=stopwords.union(load_stopwords_ascii(i))\n",
    "\n",
    "    # lowercasing all the stopwords \n",
    "    lowercase_stopwords = {word.lower() for word in stopwords}\n",
    "\n",
    "    print(\"Total stopwords extracted from files :\",len(lowercase_stopwords))\n",
    "\n",
    "    return lowercase_stopwords\n",
    "\n",
    "\n",
    "#extracting stopwords\n",
    "stopwords=set(extract_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56139c5a-b628-4eb5-830f-8683147a1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from a text\n",
    "def remove_stopwords(text, stopwords):\n",
    "    filtered_text=[]\n",
    "\n",
    "    #tokenizing each word in the text using NLTK\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text)  \n",
    "    \n",
    "    # removing stopwords from tokenized text\n",
    "    for word in text_words:\n",
    "        if word not in stopwords:\n",
    "            filtered_text.append(word)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f678d5f1-2266-426b-919f-34ae222add6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forming a column for text without stopwords\n",
    "df['text_without_stopwords']=df['article_text'].str.lower()\n",
    "\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['text_without_stopwords'][i]=' '.join(remove_stopwords(df['text_without_stopwords'][i], stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6652766-b783-4f64-adff-512bad0a98ee",
   "metadata": {},
   "source": [
    "<h3>1. Sentiment Analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4142b9-43fa-4242-bcbb-7dfd23a0babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding of the Question/MasterDictionary/negative-words.txt is: ISO-8859-1\n",
      "The encoding of the Question/MasterDictionary/positive-words.txt is: ascii\n"
     ]
    }
   ],
   "source": [
    "#Determining encoding type of text files for extracting sentiment words\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        detector = chardet.universaldetector.UniversalDetector()\n",
    "        for line in file:\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "        detector.close()\n",
    "    encoding = detector.result['encoding']\n",
    "    print(f'The encoding of the {file_path} is: {encoding}')\n",
    "\n",
    "file_path = ['Question/MasterDictionary/negative-words.txt',\n",
    "             'Question/MasterDictionary/positive-words.txt'\n",
    "            ]\n",
    "\n",
    "for i in file_path:\n",
    "    detect_encoding(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f830621-d3c4-4734-9cfa-28a5fc10eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Positive and negative words from the files present in master dictionary and lowercasing them\n",
    "with open('Question/MasterDictionary/positive-words.txt', 'r',encoding='ascii') as file:\n",
    "        positive_words=set(file.read().splitlines()) \n",
    "positive_words = {word.lower() for word in positive_words}\n",
    "\n",
    "with open('Question/MasterDictionary/negative-words.txt', 'r',encoding='ISO-8859-1') as file:\n",
    "        negative_words=set(file.read().splitlines())  \n",
    "negative_words = {word.lower() for word in negative_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e74d88e-f6b7-4080-8775-b9ac90da27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to remove URLs and punctuations\n",
    "\n",
    "import re\n",
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+\\www\\.\\S+')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "import string\n",
    "exclude=string.punctuation\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d9416a0-5f7b-4744-b9c5-ff6d0afa51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for effective NLP application for counting positive and negative words, removing URLs and punctuations\n",
    "df['preprocessed_text']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['preprocessed_text'][i] = remove_url(df['text_without_stopwords'][i])\n",
    "    df['preprocessed_text'][i] = remove_punc(df['text_without_stopwords'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba14a818-9c5a-4088-bbbd-6840f0ae05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get positive and negative score from a text\n",
    "def count_positive_score(text, words):\n",
    "    count=0\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text)  # Tokenize using nltk\n",
    "    \n",
    "    for word in text_words:\n",
    "        if word in words:\n",
    "            count=count+1\n",
    "            \n",
    "    return count\n",
    "\n",
    "def count_negative_score(text, words):\n",
    "    count=0\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text)  # Tokenize using nltk\n",
    "    \n",
    "    for word in text_words:\n",
    "        if word in words:\n",
    "            count=count-1\n",
    "\n",
    "    count = count*-1\n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a115adc9-8959-473f-b2a2-08ba9dc17ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding positive and negative score of the article text extracted\n",
    "df['positive_score']=None\n",
    "df['negative_score']=None\n",
    "\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['positive_score'][i]=count_positive_score(df['preprocessed_text'][i],positive_words)\n",
    "    df['negative_score'][i]=count_negative_score(df['preprocessed_text'][i],negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd47d5a5-99a6-49c0-ae17-11ce532d9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find find polarity score of the article text extracted\n",
    "def calc_polarity_score(positive_score,negative_score):\n",
    "    polarity_score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "    return polarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da4f42bc-f0b7-4622-91ce-90787b279ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding polarity score of the article text\n",
    "df['polarity_score']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['polarity_score'][i]=calc_polarity_score(df['positive_score'][i],df['negative_score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dcf4e25-38c6-4674-9899-b6bf72614273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating total words after cleaning for subjectivity score\n",
    "def total_words_after_cleaning(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text)\n",
    "    return len(text_words)\n",
    "\n",
    "#finding total words after cleaning in article text\n",
    "df['total_words_after_cleaning']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['total_words_after_cleaning'][i]=total_words_after_cleaning(df['preprocessed_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eec6608f-bf29-40d6-bb9d-5ffa50dc4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding subjectivity score of the article text\n",
    "def subjectivity_score(positive,negative,tot_words):\n",
    "    sub_score = (positive + negative)/ (tot_words + 0.000001)\n",
    "    return sub_score\n",
    "    \n",
    "df['subjectivity_score']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['subjectivity_score'][i]=subjectivity_score(df['positive_score'][i],df['negative_score'][i],df['total_words_after_cleaning'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89af9a-7920-4f4d-b6c1-4bedcc624266",
   "metadata": {},
   "source": [
    "<h3>2. Analysis of Readability</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05730a34-9bf6-405b-bf66-0852461bda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find number of words and sentences in a text\n",
    "def num_of_words(text):\n",
    "    text_words=remove_punc(text)\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text)\n",
    "    return len(text_words)\n",
    "\n",
    "def num_of_sent(text):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    text_sent = sent_tokenize(text)\n",
    "    return len(text_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa1e743-d322-4daa-b2d7-41543d80e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding total number of sentences and words in the article text\n",
    "df['total_num_of_words']=None\n",
    "df['total_num_of_sent']=None\n",
    "\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['total_num_of_words'][i]=num_of_words(df['article_text'][i])\n",
    "    df['total_num_of_sent'][i]=num_of_sent(df['article_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8b7789-5702-405a-957e-dec29722ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding Average Sentence Length \n",
    "df['average_sentence_length']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['average_sentence_length'][i]=df['total_num_of_words'][i]/df['total_num_of_sent'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47656482-5877-489f-ab8e-6c51b338ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find number of complex words in a text\n",
    "def complex_word_count(text):\n",
    "    \n",
    "    #removing punctuation from text and tokenizing it as well\n",
    "    text_words=remove_punc(text)\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text)\n",
    "\n",
    "    #using rules for syllables as provided for finding number of complex words\n",
    "    vowels=\"AEIOUaeiou\"\n",
    "    num_complex_words=0\n",
    "    for word in text_words:\n",
    "        count = 0\n",
    "        if not (word.endswith('es') or word.endswith('ed')):\n",
    "            for char in word:\n",
    "                if char in vowels:\n",
    "                    count=count+1\n",
    "\n",
    "        if count>2:\n",
    "            num_complex_words=num_complex_words+1\n",
    "\n",
    "    return num_complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97f0b485-30f8-4325-bf6d-21d623dcc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding number of complex words in article_text\n",
    "df['num_complex_words']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['num_complex_words'][i]=complex_word_count(df['article_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b93f81f-592f-4fa8-9949-f3030458f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding percentage of complex words in article_text\n",
    "df['percentage_of_complex_words']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['percentage_of_complex_words'][i]=df['num_complex_words'][i]/df['total_num_of_words'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76cf607e-af10-4414-93ae-0eeedeae4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding fog index of the article text\n",
    "df['fog_index']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['fog_index'][i]=0.4*(df['average_sentence_length'][i]+df['percentage_of_complex_words'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "338fa3e6-f359-4adf-b23d-de6197b96609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding average number of words per sentence\n",
    "df['avg_num_words_per_sent']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['avg_num_words_per_sent'][i]=df['total_num_of_words'][i]/df['total_num_of_sent'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "517e3236-eff3-497b-af8e-5c39057b76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove punctuation from a text\n",
    "import string\n",
    "exclude=string.punctuationu\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',exclude))\n",
    "\n",
    "# function to remove stopwords present in nltk from a text \n",
    "def remove_stopwords_by_nltk(text):\n",
    "    filtered_text=[]\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text)  # Tokenize using nltk\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    for word in text_words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            filtered_text.append(word)\n",
    "    return filtered_text\n",
    "\n",
    "# function to find word count in a text\n",
    "def word_count(text):\n",
    "    text=remove_punc(text)\n",
    "    text=remove_stopwords_by_nltk(text)\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c94691d-6243-40b4-8e47-5735ad0056d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding word count in the article\n",
    "df['word_count']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['word_count'][i]=word_count(df['article_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5fe85e5-390b-4084-8cb5-c4f7650a97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to found Syllable count per word\n",
    "def syllable_count_per_word(text):\n",
    "\n",
    "    # removing punctuation and tokenizing\n",
    "    text_words=remove_punc(text)\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text_words)\n",
    "\n",
    "    #using rules given to find number of syllables in each word\n",
    "    vowels=\"AEIOUaeiou\"\n",
    "    syllable_count=[]\n",
    "    for word in text_words:\n",
    "        count = 0\n",
    "        if not (word.endswith('es') or word.endswith('ed')):\n",
    "            for char in word:\n",
    "                if char in vowels:\n",
    "                    count=count+1\n",
    "                    \n",
    "        if count>=1:\n",
    "            syllable_count.append(count)\n",
    "        else:\n",
    "            syllable_count.append(\"No Syllables\")\n",
    "\n",
    "    return syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89d4bff3-85ba-44ba-8c74-12cfb4f01cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding syllable count per word in each article\n",
    "df['syllable_count_per_word']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['syllable_count_per_word'][i]=syllable_count_per_word(df['article_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79e433c7-7d2d-4c9d-96e7-2be831741d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find count of personal pronouns\n",
    "def count_personal_pronouns(text):\n",
    "\n",
    "    # special care taken for US country name \n",
    "    text=text.replace('US','')\n",
    "    text=text.lower()\n",
    "\n",
    "    # using regex for checking for personal pronouns\n",
    "    import re\n",
    "    pattern = r'\\b(I|we|my|ours|us)\\b'  # \\b ensures whole word matching\n",
    "\n",
    "    # Use re.findall() to find all occurrences\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    return len(matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5c71ce6-be65-4a6f-8451-b76eee6f1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding count of personal pronouns in each article\n",
    "df['count_personal_pronouns']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['count_personal_pronouns'][i]=count_personal_pronouns(df['article_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7881a90d-d2fa-421a-ba3d-a873f8cb219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find total characters in an article\n",
    "def total_char_each_word(text):\n",
    "    text_words=remove_punc(text)\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_words = word_tokenize(text)\n",
    "\n",
    "    count=0\n",
    "    for word in text_words:\n",
    "        for char in word:\n",
    "            count=count+1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e41bbe02-be36-4ef9-b8de-948a3dd978ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding average words length of an article\n",
    "df['average_word_length']=None\n",
    "for i in range(0,df.shape[0]):\n",
    "    df['average_word_length'][i]=total_char_each_word(df['article_text'][i])/df['total_num_of_words'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a5a01a6-29a4-4433-a7f7-72ea29960a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>51.833333</td>\n",
       "      <td>0.221865</td>\n",
       "      <td>20.822079</td>\n",
       "      <td>51.833333</td>\n",
       "      <td>69</td>\n",
       "      <td>214</td>\n",
       "      <td>[2, 3, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5.315113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038186</td>\n",
       "      <td>27.888889</td>\n",
       "      <td>0.158367</td>\n",
       "      <td>11.218902</td>\n",
       "      <td>27.888889</td>\n",
       "      <td>159</td>\n",
       "      <td>535</td>\n",
       "      <td>[2, 3, 2, 1, 3, 3, 2, 1, 1, 2, 2, 1, 4, 2, 1, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>4.424303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.04918</td>\n",
       "      <td>40.75</td>\n",
       "      <td>0.224949</td>\n",
       "      <td>16.38998</td>\n",
       "      <td>40.75</td>\n",
       "      <td>110</td>\n",
       "      <td>344</td>\n",
       "      <td>[2, 3, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.02863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.080882</td>\n",
       "      <td>24.777778</td>\n",
       "      <td>0.303438</td>\n",
       "      <td>10.032486</td>\n",
       "      <td>24.777778</td>\n",
       "      <td>203</td>\n",
       "      <td>458</td>\n",
       "      <td>[2, 3, 2, 1, 3, 6, 1, 1, 1, 1, 1, 2, 2, 1, No ...</td>\n",
       "      <td>4</td>\n",
       "      <td>5.765321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>19.209302</td>\n",
       "      <td>0.217918</td>\n",
       "      <td>7.770888</td>\n",
       "      <td>19.209302</td>\n",
       "      <td>180</td>\n",
       "      <td>519</td>\n",
       "      <td>[4, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 3, 2, 2, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.939467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...   \n",
       "1  Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...   \n",
       "2  Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...   \n",
       "3  Netclan20241020  https://insights.blackcoffer.com/efficient-pro...   \n",
       "4  Netclan20241021  https://insights.blackcoffer.com/development-o...   \n",
       "\n",
       "  POSITIVE SCORE NEGATIVE SCORE POLARITY SCORE SUBJECTIVITY SCORE  \\\n",
       "0              5              1       0.666667           0.033898   \n",
       "1              8              8            0.0           0.038186   \n",
       "2             11              4       0.466667            0.04918   \n",
       "3             22             11       0.333333           0.080882   \n",
       "4              9              2       0.636364           0.023013   \n",
       "\n",
       "  AVG SENTENCE LENGTH PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0           51.833333                    0.221865  20.822079   \n",
       "1           27.888889                    0.158367  11.218902   \n",
       "2               40.75                    0.224949   16.38998   \n",
       "3           24.777778                    0.303438  10.032486   \n",
       "4           19.209302                    0.217918   7.770888   \n",
       "\n",
       "  AVG NUMBER OF WORDS PER SENTENCE COMPLEX WORD COUNT WORD COUNT  \\\n",
       "0                        51.833333                 69        214   \n",
       "1                        27.888889                159        535   \n",
       "2                            40.75                110        344   \n",
       "3                        24.777778                203        458   \n",
       "4                        19.209302                180        519   \n",
       "\n",
       "                                   SYLLABLE PER WORD PERSONAL PRONOUNS  \\\n",
       "0  [2, 3, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, ...                 3   \n",
       "1  [2, 3, 2, 1, 3, 3, 2, 1, 1, 2, 2, 1, 4, 2, 1, ...                 7   \n",
       "2  [2, 3, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, ...                 1   \n",
       "3  [2, 3, 2, 1, 3, 6, 1, 1, 1, 1, 1, 2, 2, 1, No ...                 4   \n",
       "4  [4, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 3, 2, 2, 1, ...                 1   \n",
       "\n",
       "  AVG WORD LENGTH  \n",
       "0        5.315113  \n",
       "1        4.424303  \n",
       "2         5.02863  \n",
       "3        5.765321  \n",
       "4        4.939467  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forming dataframe as given in the output data structure\n",
    "\n",
    "#Renaming columns\n",
    "df=df.rename(columns={'positive_score': 'POSITIVE SCORE',\n",
    "                     'negative_score': 'NEGATIVE SCORE',\n",
    "                     'polarity_score': 'POLARITY SCORE',\n",
    "                     'subjectivity_score' : 'SUBJECTIVITY SCORE',\n",
    "                     'average_sentence_length' : 'AVG SENTENCE LENGTH',\n",
    "                     'percentage_of_complex_words' : 'PERCENTAGE OF COMPLEX WORDS',\n",
    "                     'fog_index' : 'FOG INDEX',\n",
    "                     'avg_num_words_per_sent' : 'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "                     'num_complex_words' : 'COMPLEX WORD COUNT',\n",
    "                     'word_count': 'WORD COUNT',\n",
    "                     'syllable_count_per_word' : 'SYLLABLE PER WORD',\n",
    "                     'count_personal_pronouns' : 'PERSONAL PRONOUNS',\n",
    "                     'average_word_length' : 'AVG WORD LENGTH'\n",
    "                    })\n",
    "\n",
    "#Selecting Required columns\n",
    "output_df = (df[['URL_ID',\n",
    "                'URL',\n",
    "                'POSITIVE SCORE', \n",
    "                'NEGATIVE SCORE',\n",
    "                'POLARITY SCORE', \n",
    "                'SUBJECTIVITY SCORE',\n",
    "                'AVG SENTENCE LENGTH',\n",
    "                'PERCENTAGE OF COMPLEX WORDS',\n",
    "                'FOG INDEX',\n",
    "                'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "                'COMPLEX WORD COUNT', \n",
    "                'WORD COUNT', \n",
    "                'SYLLABLE PER WORD',\n",
    "                'PERSONAL PRONOUNS', \n",
    "                'AVG WORD LENGTH']])\n",
    "\n",
    "#Final data output\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1d6516f-2aae-433d-a268-b8f9bed81b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving output to a csv file\n",
    "output_df.to_csv(\"OUTPUT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be072119-517c-4e1b-b828-6541295a53ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
